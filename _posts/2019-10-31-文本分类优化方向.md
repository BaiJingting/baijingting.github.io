---
layout: post
title: "文本分类优化方向"
date: 2019-10-31
description: ""
tag: NLP

---

##### 本文结构为：

- 为什么FastText结构简单效果也能不错
- Bert优化方向
- 常用的文本预处理tricks

------



### 一，为什么FastText结构简单效果也能不错

fastText简而言之，就是把文档中所有词通过lookup table变成向量，取平均后直接用线性分类器得到分类结果。对于短文本，特别是线性成分较大、非线性成分较小的文本分类任务，fastText可以很好的学习到其中的知识，效果不会比Bert差太多，但速度上有绝对的优势。

对于一些包含讽刺、反语的文本分类任务，包含的非线性成分更大，相比于fastText，注意力模型可以更好的捕获上下文信息。总结来说，对简单的任务，用简单的网络结构基本就够了，但是对比较复杂的任务，还是需要更复杂的网络结构来学习sentence representation的。

目前我在fastText的实验中主要优化方向为：

- 以下几个参数的调优：

```
dim                        词向量维度 default 100
ws                         上下文窗口大小 default 5
lr                         学习率 default 0.05
epoch                      epochs 数量 default 5
min_count                  最低词频 default 5
word_ngrams                n-gram 设置 default 1
loss                       损失函数 {ns,hs,softmax} default ns
```

- 文本预处理（见👇第三点）；
- kfold交叉验证；
- 结合其他一些业务上的特征（将特征和文本一同作为fastText输入）；
- 集成策略；
- Active learning（扩充训练集，减少人工标注的成本）。



### 二，Bert优化方向

从算法上和业务上，有以下几点优化方向（其他神经网络模型也类似）：

- max_length，batch_size等参数，对文本整体进行分析，选择合适的max_length尽量减少信息损失，并把有限的机器资源留给别的参数；
- 如果验证集上效果比训练集上差很多，说明出现了过拟合，加入dropout层；
- 优化方法的选择；
- 损失函数的选择；
- 基于验证集的自适应learning rate；
- Learning rate加入warmup阶段；
- Bert最后一个输出层后接入CRF、LSTM、BiLSTM等结构。（Bert、Roberta已经很大了，不建议后面再接复杂的结构，Albert可以考虑）；
- 对训练数据加入与Bert同样模式的mask（随机选择15%的token做mask，80%替换成'[MASK]', 10%保持不变，10%替换成任意token），缩小预训练与微调阶段的差异（这一点也是XLNet论文中指出的Bert的缺点），并且提升泛化能力；
- 集成策略；
- Active learning（扩充训练集，减少人工标注的成本）；
- 文本的预处理；
- 增加其他业务特征，经过特征工程后，与Bert最后一个输出层的结果拼接，共同进行下一步的预测；
- （机器资源不足的情况，也可以只对Bert的后N层进行微调）。



### 三，常用的文本预处理tricks

- 分词
- wordpiece (针对英文)
- 去停用词
- 词性筛选
- 文本泛化（链接、电话号码、邮箱、命名实体的统一替换）
- 异常数据的去除


