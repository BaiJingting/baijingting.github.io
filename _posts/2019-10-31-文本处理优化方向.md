---
layout: post
title: "文本分类优化方向"
date: 2019-10-31
description: ""
tag: NLP

---

### 一，为什么FastText结构简单效果也能不错

fastText简而言之，就是把文档中所有词通过lookup table变成向量，取平均后直接用线性分类器得到分类结果。对于短文本，特别是线性成分较大、非线性成分较小的文本分类任务，fastText可以很好的学习到其中的知识，效果不会比Bert差太多，但速度上有绝对的优势。

对于一些包含讽刺、反语的文本分类任务，包含的非线性成分更大，相比于fastText，注意力模型可以更好的捕获上下文信息。总结来说，对简单的任务，用简单的网络结构基本就够了，但是对比较复杂的任务，还是需要更复杂的网络结构来学习sentence representation的。

目前我在fastText的实验中主要优化方向为：

- 以下几个参数的调优：

```
dim                        词向量维度 default 100
ws                         上下文窗口大小 default 5
lr                         学习率 default 0.05
epoch                      epochs 数量 default 5
min_count                  最低词频 default 5
word_ngrams                n-gram 设置 default 1
loss                       损失函数 {ns,hs,softmax} default ns
```

- 文本预处理（见👇第四点）；
- kfold交叉验证；
- 结合其他一些业务上的特征（将特征和文本一同作为fastText输入）；
- 集成策略。



### 二，Bert优化方向

从算法上和业务上，有以下几点优化方向：

- max_length，batch_size等参数，对文本整体进行分析，选择合适的max_length尽量减少信息损失，并把有限的机器资源留给别的参数；
- 如果验证集上效果比训练集上差很多，说明出现了过拟合，加入dropout层；
- 优化方法的选择；
- 损失函数的选择；
- 基于验证集的自适应learning rate；
- Learning rate加入warmup阶段；
- Bert最后一个输出层后接入CRF、LSTM、BiLSTM等结构。（Bert、Roberta已经很大了，不建议后面再接复杂的结构，Albert可以考虑）；
- 对训练数据加入与Bert同样模式的mask（随机选择15%的token做mask，80%替换成'[MASK]', 10%保持不变，10%替换成任意token），缩小预训练与微调阶段的差异（这一点也是XLNet论文中指出的Bert的缺点），并且提升泛化能力；
- 集成策略；
- 文本的预处理；
- 增加其他业务特征，经过特征工程后，与Bert最后一个输出层的结果拼接，共同进行下一步的预测；
- （机器资源不足的情况，也可以只对Bert的后N层进行微调）。



### 三，Bert、Roberta、Ernie和Albert的对比

**Bert**：[Bert原理及源码解读](https://baijingting.github.io/2019/10/Bert原理及源码解读/)

**RoBERTa** (A Robustly Optimized BERT) 是 BERT 的改进版，相比于Bert，主要改进为：

- 模型规模、算力和数据上，更大的模型参数量，更大bacth size，更多的训练数据；
- 训练方法上，去掉 NSP 任务，使用动态掩码。

**Ernie** 百度出的Bert改进版，主要改进为：

- 对中文进行分词（不清楚在相同训练预料的情况下，分词是否真的能带来效果上的提升，个人认为，分词本身会带来误差累计，随着Transformer的能力越来越强大，分词的特征应该让Transformer当做内部特征去学习，绝大多数任务使用端到端的方式应该会更好，所以感觉中文分词是不必要存在的）；
- 训练语料引入了多源数据知识。除了百科类文章建模，还对新闻资讯类、论坛对话类数据。

**Albert** ( A Lite Bert)： [Albert原理及源码解读](https://baijingting.github.io/2019/10/Albert原理及源码解读/)，相比于Bert，主要改进为：

- Factorized Embedding Parameterization：词表的embedding size (E) 和transformer层的hidden size (H) 分开设置；
- Cross-layer parameter sharing，仅共享attention的参数；
- Inter-sentence coherence loss：使用MLM和SOP任务。

Albert的Factorized Embedding Parameterization主要是大幅减少模型参数量，提升预训练速度，但从效果上相比于Bert是略微变差的（base对base，large对large），但是模型规模的变小让训练Albert xxlarge更容易，所以整体效果是提升的。

（个人理解）Bert的NSP任务比Albert的SOP任务更适合两句相似度判断的场景（比如query和document的匹配程度、问答中相似问题），NSP倾向于判断两句是否有主题上的关联，SOP则学习主题以外的是否为上下句的特征。在 [平安医疗科技疾病问答迁移学习比赛](https://www.biendata.com/competition/chip2019/) 中，实验结果也验证了我的想法：

| 模型         | 线上F1 |
| ------------ | ------ |
| Bert         | 84.58% |
| Roberta      | 85.59% |
| Albert-large | 83.94% |

Albert论文的实验里也说明，有些数据集和任务上，Bert表现更优，有些Albert更优。



### 四，常用的文本预处理tricks

- 分词
- wordpiece (针对英文)
- 去停用词
- 词性筛选
- 文本泛化（链接、电话号码、邮箱、命名实体的统一替换）
- 异常数据的去除


