---
layout: post
title: "文本分类优化方向"
date: 2019-10-31
description: ""
tag: NLP

---

### 1，为什么FastText结构简单效果也能不错

fastText简而言之，就是把文档中所有词通过lookup table变成向量，取平均后直接用线性分类器得到分类结果。对于短文本，特别是线性成分较大、非线性成分较小的文本分类任务，fastText可以很好的学习到其中的知识，效果不会比Bert差太多，但速度上有绝对的优势。

对于一些包含讽刺、反语的文本分类任务，包含的非线性成分更大，相比于fastText，注意力模型可以更好的捕获上下文信息。总结来说，对简单的任务，用简单的网络结构基本就够了，但是对比较复杂的任务，还是需要更复杂的网络结构来学习sentence representation的。

目前我在fastText的实验中主要优化方向为文本预处理（见👇第5点），及以下几个参数的调优：

```
dim                        词向量维度 default 100
ws                         上下文窗口大小 default 5
epoch                      epochs 数量 default 5
min_count                  最低词频 default 5
word_ngrams                n-gram 设置 default 1
```



### 2，Bert优化方向

1，如果验证集上效果比训练集上差很多，说明出现了过拟合，加入dropout层；

2，优化方法的选择；

3，max_length，batch_size等参数，选择合适的max_length尽量减少信息损失，并把有限的机器资源留给别的参数；

4，Bert后接入CRF、LSTM、BiLSTM等结构（我木有试过，不知道是否能提升效果，感觉Bert的12/24层transformer已经够强大了）；

5，文本预处理（见👇第5点）。



### 3，Bert、Roberta和Albert的对比

**Bert**：[Bert原理及源码解读](https://baijingting.github.io/2019/10/Bert原理及源码解读/)

**RoBERTa** (A Robustly Optimized BERT) 是 BERT 的改进版，相比于Bert，主要改进为：

- 模型规模、算力和数据上，更大的模型参数量，更大bacth size，更多的训练数据；
- 训练方法上，去掉 NSP 任务，使用动态掩码。

**Albert**( A Lite Bert)： [Albert原理及源码解读](https://baijingting.github.io/2019/10/Albert原理及源码解读/)

- Factorized Embedding Parameterization：词表的embedding size (E) 和transformer层的hidden size (H) 分开设置；
- Cross-layer parameter sharing，仅共享attention的参数；
- Inter-sentence coherence loss：使用MLM和SOP任务。

Albert的Factorized Embedding Parameterization主要是大幅减少模型参数量，提升预训练速度，但从效果上相比于Bert是略微变差的（base对base，large对large），但是模型规模的变小让训练Albert xxlarge更容易，所以整体效果是提升的。

（个人理解）Bert的NSP任务比Albert的SOP任务更适合两句相似度判断的场景（比如query和document的匹配程度、问答中相似问题），NSP倾向于判断两句是否有主题上的关联，SOP则学习主题以外的是否为上下句的特征。在 [平安医疗科技疾病问答迁移学习比赛](https://www.biendata.com/competition/chip2019/) 中，实验结果也验证了我的想法：

| 模型         | 线上F1 |
| ------------ | ------ |
| Bert         | 84.58% |
| Roberta      | 85.59% |
| Albert-large | 83.94% |

Albert论文的实验里也说明，有些数据集和任务上，Bert表现更优，有些Albert更优。



### 4，常用的文本预处理tricks

- 分词
- wordpiece (针对英文)
- 去停用词
- 词性筛选
- 文本泛化（链接、电话号码、邮箱、命名实体的统一替换）

