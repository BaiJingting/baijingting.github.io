#### Bert网络结构

<img src="https://github.com/BaiJingting/baijingting.github.io/blob/master/images/posts/image-20191008154841673.png?raw=true" alt="image-20191008154841673" style="zoom:80%;" />

- BERT用transformer方法取代了ELMo中用LSTM提取特征的方法；
- BERT解决了GPT中单向语言模型的方法，变为双向；
- BERT采用了Fine tuning方式。（ELMo为Feature-based：将训练出的representation作为feature用于任务，词向量、句向量、段向量、文本向量都是这样的）

Bert提供了简单和复杂两个模型，对应的超参数分别如下：

- $$BERT_{BASE}$$： L=12, H=768, A=12，参数总量110M；
- $$BERT_{LARGE}$$： L=24, H=1024, A=16，参数总量340M；

其中，L表示网络的层数，即Transformer blocks的数量，A表示Multi-Head Attention的数量

##### Transformer

<img src="https://github.com/BaiJingting/baijingting.github.io/blob/master/images/posts/image-20191008161255984.png?raw=true" alt="image-20191008161255984" style="zoom:67%;" />

Transformer为encoder-decoder​结构，如上图所示，左半部分为encoder，右半部分为decoder。

**Encoder**：

​		encoder由6个相同的layer组成，每个layer由两个sub-layer组成，分别是multi-head self-attention mechanism和fully connected feed-forward network。其中每个sub-layer都加了residual connection 和 normalisation，因此可以将sub-layer的输出表示为：

​					$$ sub\_layer\_output = LayerNorm(x + (SubLayer(x))) $$

1. Multi-head self-attention mechanism

   attention可表示为：$$ attention\_output = Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}}V) $$

   multi-head attention是通过h个不同的线性变换对Q，K，V进行投影，最后将不同的attention结果拼接起来：

   ​        $$ MultiHead(Q, K, V) = Concat(head_1, \cdots, head_h) W^o$$

   ​        $$ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$

   Self-attention则是取Q，K，V相同。

2. Position-wise feed-forward networks

   全联接层。

#### Bert特点

- 基于self-attention的Transformer结构。
- 使用Masked Language Model (masked LM)和Next Sentence Prediction (NSP)两个任务共同参与预训练过程，loss为两个任务的loss之和。
- MLM任务中，随机mask15%的token，而不是像cbow一样把每个词都预测一遍。损失函数只计算被mask掉的token的损失。mask的token中80%被替换为[MASK]，10%被替代成其他单词，10%的token不做替换。

------

下面从源码出发对Bert进行详细介绍。

#### Bert输入与预处理

```python
input_ids = tf.constant([[31, 51, 99], [15, 5, 0]]) 
（每个字符在字典中的ID，输入之后embedding层会转换成向量，使用标准差为0.02的截断正态分布初始化768维的向量）
input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])
```

#####  tokenization.py

- **FullTokenizer**

  Bert 里的分词主要由FullTokenizer类来实现。调用BasicTokenizer和WordpieceTokenizer。前者是根据空格、标点等进行普通的分词，而后者会把前者的结果再细粒度的切分为WordPiece。

  ```python
  class FullTokenizer(object):
      """Runs end-to-end tokenziation."""
  
      def __init__(self, vocab_file, do_lower_case=True):
          self.vocab = load_vocab(vocab_file)
          self.inv_vocab = {v: k for k, v in self.vocab.items()}
          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
          self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)
      
      def tokenize(self, text):
          split_tokens = []
          for token in self.basic_tokenizer.tokenize(text):
              for sub_token in self.wordpiece_tokenizer.tokenize(token):
                  split_tokens.append(sub_token)
          return split_tokens
  
      def convert_tokens_to_ids(self, tokens):
          return convert_by_vocab(self.vocab, tokens)
  
      def convert_ids_to_tokens(self, ids):
          return convert_by_vocab(self.inv_vocab, ids)
  ```

- **BasicTokenizer** : 根据空格、标点等进行普通的分词，以及对汉字切分为字。

  ```python
  def _is_chinese_char(self, cp):
      """判断一个unicode字符是否是汉字"""
      if ((cp >= 0x4E00 and cp <= 0x9FFF) or 
          (cp >= 0x3400 and cp <= 0x4DBF) or 
          (cp >= 0x20000 and cp <= 0x2A6DF) or 
          (cp >= 0x2A700 and cp <= 0x2B73F) or
          (cp >= 0x2B740 and cp <= 0x2B81F) or
          (cp >= 0x2B820 and cp <= 0x2CEAF) or
          (cp >= 0xF900 and cp <= 0xFAFF) or
          (cp >= 0x2F800 and cp <= 0x2FA1F)):
          return True
      return False
  
  def _run_strip_accents(self, text):
      """去除accents"""
      text = unicodedata.normalize("NFD", text)
      output = []
      for char in text:
          cat = unicodedata.category(char)
          if cat == "Mn":
              continue
          output.append(char)
      return "".join(output)
  ```

- **WordpieceTokenizer** : 对英文单词切分为细粒度的WordPiece。（Bert做mask时，对属于同一个单词的WordPiece同时mask）

  ```python
  def tokenize(self, text):
  		"""
      使用贪心的最大正向匹配算法把一个词切分成word piece。##表示这个词是接着前面的，WordPiece切分是可逆的——我们可以恢复出“真正”的词。
      """
      text = convert_to_unicode(text)
  
      output_tokens = []
      for token in whitespace_tokenize(text):
          chars = list(token)
          if len(chars) > self.max_input_chars_per_word:
              output_tokens.append(self.unk_token)
              continue
  
          is_bad = False
          start = 0
          sub_tokens = []
          while start < len(chars):
              end = len(chars)
              cur_substr = None
              while start < end:
                  substr = "".join(chars[start:end])
                  if start > 0:
                      substr = "##" + substr
                  if substr in self.vocab:
                      cur_substr = substr
                      break
                  end -= 1
              if cur_substr is None:
                  is_bad = True
                  break
              sub_tokens.append(cur_substr)
              start = end
  
          if is_bad:
              output_tokens.append(self.unk_token)
          else:
              output_tokens.extend(sub_tokens)
      return output_tokens
  ```

##### create_pretraining_data.py

这块最重要的操作是函数create_instances_from_document中的内容，从单个文档中生成由TrainingInstance对象组成的instances列表，涉及到Masked Language Model (masked LM)和Next Sentence Prediction (NSP)数据准备的具体实现细节。

```python
def create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,
                                   masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
    """从单个文档生成TrainingInstance对象"""
    document = all_documents[document_index]

    # 预留[CLS], [SEP], [SEP]的位置
    max_num_tokens = max_seq_length - 3

    # 增加一定的随机性，并减少pre-training和fine-tuning的差别，设置target_seq_length，但target_seq_length只是粗略目标，
    # 最终的长度不一定会短于target_seq_length，但一定不会超过max_seq_length
    target_seq_length = max_num_tokens
    if rng.random() < short_seq_prob:
        target_seq_length = rng.randint(2, max_num_tokens)

    # 根据document中的sentences，获得segment_A和segment_B，(一个segment可能只有一个句子，也可能由多个句子拼接而成)
    instances = []
    current_chunk = []
    current_length = 0
    i = 0
    while i < len(document):
        segment = document[i]
        current_chunk.append(segment)
        current_length += len(segment)
        if i == len(document) - 1 or current_length >= target_seq_length:
            if current_chunk:
                # a_end表示current_chunk中有多少segments会进入sengment_A
                a_end = 1
                if len(current_chunk) >= 2:
                    a_end = rng.randint(1, len(current_chunk) - 1)

                tokens_a = []
                for j in range(a_end):
                    tokens_a.extend(current_chunk[j])

                tokens_b = []
                is_random_next = False
                if len(current_chunk) == 1 or rng.random() < 0.5:
                    is_random_next = True
                    target_b_length = target_seq_length - len(tokens_a)

                    # 随机选择其他文档
                    for _ in range(10):
                        random_document_index = rng.randint(0, len(all_documents) - 1)
                        if random_document_index != document_index:
                            break

                    random_document = all_documents[random_document_index]
                    random_start = rng.randint(0, len(random_document) - 1)
                    for j in range(random_start, len(random_document)):
                        tokens_b.extend(random_document[j])
                        if len(tokens_b) >= target_b_length:
                            break
                    # 确定current_chunk中未用到的句子，并将遍历该文档句子的索引重新置位到未用到的句子的位置。不浪费句子中的信息。
                    num_unused_segments = len(current_chunk) - a_end
                    i -= num_unused_segments
                else:
                    # 使用真实的下一句
                    is_random_next = False
                    for j in range(a_end, len(current_chunk)):
                        tokens_b.extend(current_chunk[j])
                # 选择tokens_a和tokens_b中较长的做截断，随机从头或尾做截断
                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)

                assert len(tokens_a) >= 1
                assert len(tokens_b) >= 1

                tokens = []
                segment_ids = []
                tokens.append("[CLS]")
                segment_ids.append(0)
                for token in tokens_a:
                    tokens.append(token)
                    segment_ids.append(0)

                tokens.append("[SEP]")
                segment_ids.append(0)

                for token in tokens_b:
                    tokens.append(token)
                    segment_ids.append(1)
                tokens.append("[SEP]")
                segment_ids.append(1)
								# 随机选择15%的token做mask，80%替换成'[MASK]', 10%保持不变，10%替换成任意token
                (tokens, masked_lm_positions,masked_lm_labels) \
                    = create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
                instance = TrainingInstance(
                                tokens=tokens,
                                segment_ids=segment_ids,
                                is_random_next=is_random_next,
                                masked_lm_positions=masked_lm_positions,
                                masked_lm_labels=masked_lm_labels)
                instances.append(instance)
            current_chunk = []
            current_length = 0
        i += 1
    return instances
```

#### Bert 模型代码

#### 预训练



#### 微调
